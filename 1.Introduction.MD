# Introduction

## Scrapy at a glance
**Functions**: **crawling web sites** and **extracting structured data**.
> These data can be used for a wide range of useful applications, like data mining, information processing or historical archival.

### Walk-through of an example spider (The first program)
Walking you through an example of a Scrapy Spider using the simplest way to run a spider.  

The spider that scrapes famous quotes from website http://quotes.toscrape.com.
```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = 'quotes'
    start_urls = [
        'http://quotes.toscrape.com/tag/humor/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'author': quote.xpath('span/small/text()').get(),
                'text': quote.css('span.text::text').get(),
            }

        next_page = response.css('li.next a::attr("href")').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```
Run the spider using the `runspider` command:  
`scrapy runspider quotes_spider.py -o quotes.json`  

**The process** of the program: The crawl started by making requests to the URLs defined in the  `start_urls` attribute (in this case, only the URL for quotes in humor category) and called the default callback method parse, passing the response object as an argument. In the parse callback, we loop through the quote elements using a CSS Selector, yield a Python dict with the extracted quote text and author, look for a link to the next page and schedule another request using the same parse method as callback.

The **main advantages** about Scrapy: Requests are scheduled and processed *asynchronously*. This means that Scrapy doesn’t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime. This also means that other requests can keep going even if some request fails or an error happens while handling it.

## Other functions of Scrapy
Scrapy provides a lot of powerful features for making scraping easy and efficient, such as:
- Built-in support for selecting and extracting data from HTML/XML sources using extended *CSS selectors* and *XPath* expressions, with helper methods to extract using *regular expressions*.
- An interactive shell console (IPython aware) for trying out the CSS and XPath expressions to scrape data, very useful when writing or debugging your spiders.
- Built-in support for generating feed exports in multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP, S3, local filesystem).
- Robust encoding support and auto-detection, for dealing with foreign, non-standard and broken encoding declarations.
- Strong extensibility support, allowing you to plug in your own functionality using signals and a well-defined API (middlewares, extensions, and pipelines).
- Wide range of built-in extensions and middlewares for handling:
  - cookies and session handling
  - HTTP features like compression, authentication, caching
  - user-agent spoofing
  - robots.txt
  - crawl depth restriction
  - and more ......
- A Telnet console for hooking into a Python console running inside your Scrapy process, to introspect and debug your crawler.
- Plus other goodies like reusable spiders to crawl sites from Sitemaps and XML/CSV feeds, a media pipeline for automatically downloading images (or any other media) associated with the scraped items, a caching DNS resolver, and much more!

## What’s next?
The next steps for you are to install Scrapy, follow through the tutorial to learn how to create a full-blown Scrapy project and join the community. Thanks for your interest!